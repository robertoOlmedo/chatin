# -*- coding: utf-8 -*-
"""recomender.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hmz15b7R8B1d-nk4i9G4JgXxBsFvNKP_
"""



# -*- coding: utf-8 -*-
"""
Created on Mon May 24 08:19:22 2021

@author: Tinamica
"""
"""A liveness prober dag for monitoring composer.googleapis.com/environment/healthy."""

# IMPORTAMOS LIBRERIAS
import airflow
from airflow import DAG
from airflow import models
from airflow.operators import bash_operator, python_operator
from airflow.contrib.operators.bigquery_operator import BigQueryOperator
from datetime import timedelta, datetime, date
import pandas_gbq
import csv
import pandas as pd
from collections import Counter
from google.cloud import bigquery
from sklearn.preprocessing import MinMaxScaler
import seaborn as sns
import matplotlib.pyplot as plt
import random as rd
import re
import numpy as np
import gcsfs
import requests

entorno = 'PROD'
if entorno == 'PROD':
    libreria = ''
else: 
    libreria = '_TEST'


def puntuaciones(**kwargs):
    #####LOADING

    client = bigquery.Client(project="jft-content-recommender")

    conten = client.query('''
    select distinct  * except(CD_AEROPUERTOS) , CD_AEROPUERTO AS CD_AEROPUERTOS,
       case when pases_hora =0 then 0 
            when Total_pases_comprados =0 then 0 
       else  ceil(Total_pases_comprados / pases_hora) end as huecos_comprados, 
    -- Total_pases_comprados es aquÃ­ los pases comprados a la semana
      from(
        SELECT  * except(	huecos_comprados	,Total_pases_comprados) , 
        -- si no es la primera vez que estÃ¡ el contenido: forecast !=0
        case when forecast !=0 and forecast > 0 
        then coalesce(forecast - coalesce(emitido_semana,0),0) 
        when forecast =-99 then -99
        -- si es la primera vez que se emite el contenido forecast = 0 
        else  (pases_contratados/dias_emision_totales) * dias_emision_semana  end Total_pases_comprados,

        from(
        --por aqui no estoy separando en sala y aeropuerto porq el contenido se emite las mismas veces todo el tiempo (a no ser que se suba el mismo contenido para otra sala, pero tendrÃ­a el nombre cambiado)
          select a.*, 
          pases_contratados, semana_inicio, semana_fin , b.forecast, b.emitido_semana,
          DATE_DIFF( semana_fin,date_add(CURRENT_DATE(), interval 1 DAY),  DAY) +1 dias_emision_semana

          FROM  
          (select * except(MR_MOMENTO_MES	,MR_TEMPORADA), DATE_DIFF(fecha_baja,fecha_alta,  DAY) + 1 dias_emision_totales
           from `jft-content-recommender.JFT_DATOS_PROD''' + libreria + '''.Maestro_contenidos_caracteristicas`) a 
          left join  
          -- ## como a veces da error en los players cogemos los players de cada contenido que mÃ¡s emision tienen,
           --porque ese serÃ­a el nÃºmero de emisiones en la sala. Solo perdemos las emisiones de los 
           --players que no hayan funcionado bien 
          (select * except(row) from (select semana_inicio, semana_fin,   nombre_contenido, pases_contratados,  forecast,  emitido_semana , ROW_NUMBER() OVER ( PARTITION BY 	semana_inicio	,semana_fin	,nombre_contenido 
          ORDER BY emitido_acumulado desc, emitido_semana desc ) row
          from `jft-content-recommender.JFT_DATOS_PROD''' + libreria + '''.Maestro_ocupacion` order by semana_inicio)
          where row=1)  b using(nombre_contenido) 
          where 1=1
            and date_add(CURRENT_DATE(), interval 1 DAY) >= semana_inicio and date_add(CURRENT_DATE(), interval 1 DAY) <= semana_fin
          and a.MR_CAMPAIGN ="SI" and  replace(a.subcategoria, " ","")  = "Anunciantes-CampaÃ±as" ) 
          order by semana_inicio)  ''').to_dataframe()
    semana = client.query(

        '''with fecha_max as(SELECT max(fecha_carga)  fecha_carga FROM `JFT_DATOS_PROD''' + libreria + '''.Maestro_semanal_caracteristicas`)
         SELECT *except(MR_PAIS_ORIGEN, ASIENTOS, MIN_hemis_menos_hvuelo),coalesce(MR_PAIS_ORIGEN, "") MR_PAIS_ORIGEN, 
         coalesce(ASIENTOS, "0") ASIENTOS, coalesce(MIN_hemis_menos_hvuelo, "0") MIN_hemis_menos_hvuelo,
         MR_TIPO_VUELO
         
         FROM `JFT_DATOS_PROD''' + libreria + '''.Maestro_semanal_caracteristicas` inner join fecha_max using (fecha_carga)
        ''').to_dataframe()
    ### cojo solo los dias de la semana hasta el lunes
    pesos = client.query('''
SELECT * FROM `JFT_DATOS_PROD''' + libreria + '''.Datos_excel_pesos` where criterio not in ('MR_CAMPAÃ‘A','MR_MOTIVO_VIAJE', 'MR_TEMPORADA, MR_MOMENTO_MES')  ''').to_dataframe()

    conten.huecos_comprados = conten.huecos_comprados.astype("int")
    conten = conten[(conten['CD_AEROPUERTOS'].notna()) & (conten['CD_SALAS'].notna())]
    ###exijo que haya sala y aeropuerto

    semana["MR_PAIS_ORIGEN"] = semana["MR_PAIS_ORIGEN"].apply(lambda x: x.split(','))
    semana["ASIENTOS"] = semana["ASIENTOS"].apply(lambda x: x.split(','))
    semana["MIN_hemis_menos_hvuelo"] = semana["MIN_hemis_menos_hvuelo"].apply(lambda x: x.split(','))


    #### CÃLCULO PUNTUACIONES
    col_to_check = ["CD_AEROPUERTOS", "CD_SALAS", 'MR_CLIMA_TEMPERATURA', 'MR_LLUVIA', 'MR_PAIS_ORIGEN',
                    'MR_MOMENTO_DEL_DIA', 'MR_DIA_SEMANA','MR_TIPO_VUELO']

    # convierto en listas
    for col in col_to_check:
        conten[col] = conten[col].apply(lambda x: x.split(',') if x else [])
    # cross join
    conten = conten.explode("CD_AEROPUERTOS").explode("CD_SALAS")
    conten["MOV"] = conten["CD_SALAS"].apply(
        lambda row: "SALIDA" if re.search(r'SALID|EMBARQ|DEPART', row) else "LLEGADA" if re.search(r'LLEGAD|ARRIV',
                                                                                                   row) else None)
    puntuaciones = semana[
        ["fecha", "hora", "CD_AEROPUERTOS", "MOV", "inicio_calendario_campaign", "fin_calendario_campaign"]].copy()

    nombre_contenidos = pd.DataFrame(conten[
                                         ["CD_AEROPUERTOS", 'CD_SALAS', 'MOV', "nombre_contenido", "huecos_comprados",
                                          "pases_hora", "fecha_alta", "fecha_baja"]].copy())
    # tabla cada contenido con las correspondientes fechas, aerop y salas
    

    puntuaciones = pd.merge(left=puntuaciones, right=nombre_contenidos, on=['CD_AEROPUERTOS', 'MOV'], how='inner')

    puntuaciones['fecha'] = pd.to_datetime(puntuaciones['fecha'])
    
    puntuaciones['fecha_alta2'] = pd.to_datetime(puntuaciones['fecha_alta']).dt.date
    puntuaciones['fecha_baja2'] = pd.to_datetime(puntuaciones['fecha_baja']).dt.date

    puntuaciones = puntuaciones[
        (puntuaciones.fecha_alta2 <= puntuaciones.fecha) & (puntuaciones.fecha_baja2 >= puntuaciones.fecha)].reset_index(
        drop=True)

    puntuaciones.sort_values(["nombre_contenido", "fecha", "hora", "MOV"]).head(10)
    ###tienes cada contenido en su sala correspondiente, ejemplo estÃ¡ en dos salas pues como si fueran dos, de momento simulamos que todas las salas de llegada son iguales,
    # por tanto como hay unas 6 habrÃ¡ 6 puntuaciones que seran iguales, pero como al distribuir cuando coge una "tacha" las demas x mismo dia y hora no hay problema.

    col_to_check = ['MR_CLIMA_TEMPERATURA', 'MR_LLUVIA', 'MR_PAIS_ORIGEN', 'MR_MOMENTO_DEL_DIA', 'MR_DIA_SEMANA','MR_TIPO_VUELO']

    l_paises = ["ES", "GB", "DE", "BE", "FR", "NL", "IE", "IT", "DK", "FI", "NO", "SE"]

    # display(puntuaciones[["nombre_contenido","CD_SALAS","CD_AEROPUERTOS"]].drop_duplicates())
    semana['fecha'] = pd.to_datetime(semana['fecha'])
    for col in col_to_check:
        puntuaciones[col] = 0

        for row in range(puntuaciones.shape[0]):

            # display(puntuaciones.iloc[row])
            # display(conten.loc[(conten.nombre_contenido==puntuaciones.nombre_contenido.iloc[row])&(conten.CD_SALA==puntuaciones.CD_SALA.iloc[row])&(conten.CD_AEROPUERTOS==puntuaciones.CD_AEROPUERTOS.iloc[row])])
            # display(semana.loc[((semana.fecha==puntuaciones.fecha.iloc[row])&(semana.hora==puntuaciones.hora.iloc[row])&(semana.CD_SALA==puntuaciones.CD_SALA.iloc[row])&(semana.CD_AEROPUERTOS==puntuaciones.CD_AEROPUERTOS.iloc[row]))])
            # coge el contenido que se corresponde por players , titulo y aeropuerto

            contens = conten.loc[(conten.nombre_contenido == puntuaciones.nombre_contenido.iloc[row]) & (
                    conten.CD_SALAS == puntuaciones.CD_SALAS.iloc[row]) & (
                                         conten.CD_AEROPUERTOS == puntuaciones.CD_AEROPUERTOS.iloc[row]), col].iloc[
                0]  # lista carac para cada contenido por columna
            # coge la semana que se corresponde por fecha , hora , aeropuerto y salas

            semana_pr = semana.loc[((semana.fecha == puntuaciones.fecha.iloc[row]) & (
                    semana.hora == puntuaciones.hora.iloc[row]) & (semana.MOV == puntuaciones.MOV.iloc[row]) & (
                                            semana.CD_AEROPUERTOS == puntuaciones.CD_AEROPUERTOS.iloc[
                                        row])), col].iloc[0]  # lista carac semanal para cada fecha/hora
            # print(contens)
            # print(semana_pr)

            if (type(semana_pr) != list):
                semana_pr = [semana_pr]

            if col == "MR_PAIS_ORIGEN":
                asientos_l = semana.loc[((semana.fecha == puntuaciones.fecha.iloc[row]) & (
                        semana.hora == puntuaciones.hora.iloc[row]) & (semana.MOV == puntuaciones.MOV.iloc[row]) & (
                                                 semana.CD_AEROPUERTOS == puntuaciones.CD_AEROPUERTOS.iloc[
                                             row])), "ASIENTOS"].iloc[0]
                score = sum(int(asientos_l[sem_posib_i]) for conten_posib_i in range(len(contens)) for sem_posib_i in
                            range(len(semana_pr)) if semana_pr[sem_posib_i] in contens[conten_posib_i])
                if "OP" in contens:
                    # sumo todos los asientos de paises que no estan en la lista
                    score = score + sum(
                        int(asientos_l[sem_posib_i]) for conten_posib_i in range(len(contens)) for sem_posib_i in
                        range(len(semana_pr)) if semana_pr[sem_posib_i] not in l_paises)
            else:
                score = sum(1 for conten_posib in contens for sem_posib in semana_pr if sem_posib in conten_posib)

            # print(score)

            puntuaciones.loc[row, col] = score

    puntuaciones.sort_values(["nombre_contenido", "fecha", "hora", "MOV"]).head(10)
    sns.histplot(data=puntuaciones[puntuaciones["MR_PAIS_ORIGEN"] != 0], x="MR_PAIS_ORIGEN", bins=50)
    plt.show()
    puntuaciones["MR_PAIS_ORIGEN"] = np.log10(1 + puntuaciones["MR_PAIS_ORIGEN"])
    sns.histplot(data=puntuaciones[puntuaciones["MR_PAIS_ORIGEN"] != 0], x="MR_PAIS_ORIGEN", bins=50)
    plt.show()

    def minmaxsc(x):
        xmin = 0
        xmax = 3.5
        y = (x - xmin) / (xmax - xmin)
        return (y)

    ### hace el logaritmo para que la escala al normalizar tenga mÃ¡s sentido., con esa escala decidimos multiplicar por 2 en los pesos.

    puntuaciones.loc[puntuaciones["MR_PAIS_ORIGEN"] > 3.5, "MR_PAIS_ORIGEN"] = 3.5
    puntuaciones["MR_PAIS_ORIGEN"] = minmaxsc(puntuaciones["MR_PAIS_ORIGEN"])

    sns.histplot(data=puntuaciones[puntuaciones["MR_PAIS_ORIGEN"] != 0], x="MR_PAIS_ORIGEN", bins=50)
    plt.show()
    # procesamiento puntuaciones

    for row in range(pesos.shape[0]):
        puntuaciones[pesos.loc[row, "Criterio"]] = puntuaciones[pesos.loc[row, "Criterio"]] * pesos.loc[
            row, "peso_norm"]

    puntuaciones["TOTAL"] = puntuaciones[col_to_check].sum(axis=1)

    puntuaciones["fecha_carga"] = datetime.now()
    # display(puntuaciones.info())

    if puntuaciones.shape[0] != 0:
        puntuaciones['fecha'] = puntuaciones['fecha'].dt.strftime('%Y-%m-%d')
        puntuaciones = puntuaciones[['nombre_contenido',
        'fecha',
        'fecha_carga',
        'hora',
        'CD_AEROPUERTOS',
        'MOV',
        'CD_SALAS',
        'huecos_comprados',
        'pases_hora',
        'fecha_alta',
        'fecha_baja',
        'MR_CLIMA_TEMPERATURA',
        'MR_LLUVIA',
        'MR_PAIS_ORIGEN',
        'MR_MOMENTO_DEL_DIA',
        'MR_DIA_SEMANA',
        'TOTAL',
        'inicio_calendario_campaign',
        'fin_calendario_campaign',
        'MR_TIPO_VUELO']]
        puntuaciones.to_gbq("JFT_CR_PROD" + libreria + ".puntuaciones", "jft-content-recommender", if_exists="append")
    else:
        print("Tabla de puntuaciones vacÃ­a")
        
def por_aeropuerto(**kwargs):
    client = bigquery.Client(project="jft-content-recommender")
    
    aeropuertos = client.query("""
    with fecha_max as(SELECT distinct max(fecha_carga)  fecha_carga FROM `JFT_CR_PROD""" + libreria + """.puntuaciones`),
    seleccion as (select distinct CD_AEROPUERTOS, fecha_carga
    FROM `JFT_CR_PROD""" + libreria + """.puntuaciones` inner join fecha_max using (fecha_carga))
     select distinct CD_AEROPUERTOS from seleccion """).to_dataframe()
    return aeropuertos


def reparto_aeropuerto(AEROPUERTO):
    client = bigquery.Client(project="jft-content-recommender")

    puntuaciones = client.query("""
    with fecha_max as(SELECT max(fecha_carga)  fecha_carga FROM `JFT_CR_PROD""" + libreria + """.puntuaciones`),
    seleccion as (
 
    SELECT *except(fecha,hora,inicio_calendario_campaign,fin_calendario_campaign), date(fecha)fecha ,parse_time('%H:%M:%S', hora) hora,
    if(date(inicio_calendario_campaign) > date(fecha_alta),date(inicio_calendario_campaign),date(fecha_alta)) inicio_calendario_campaign,
    if(date(fin_calendario_campaign) < date(fecha_baja),date(fin_calendario_campaign),date(fecha_baja)) fin_calendario_campaign,
    FROM `JFT_CR_PROD""" + libreria + """.puntuaciones` inner join fecha_max using (fecha_carga))
     select * from seleccion where huecos_comprados >=0 AND
     
     CD_AEROPUERTOS ='""" + AEROPUERTO + """'""" ).to_dataframe()

    # creaciÃ³n de dataset de resultados

    resultados = puntuaciones[["CD_AEROPUERTOS", "CD_SALAS", "fecha", "hora"]].drop_duplicates()
    # print("total huecos disponibles", puntuaciones[["fecha","hora"]].drop_duplicates().shape[0])
    resultados['nombre_contenido'] = ''
    resultados['nombre_contenido'] = resultados['nombre_contenido'].apply(list)
    resultados['score'] = ''
    resultados['score'] = resultados['score'].apply(list)
    # el gestoer no nos da la capacidad para el mismo contenido diferente sala , asique lo pondremos en todo el aeropuerto, priorizando la sala con mayor puntuaciÃ³n
    # creaciÃ³n columnas de cÃ¡lculo
    puntuaciones["huecos_a_rellenar"] = puntuaciones["huecos_comprados"]
    puntuaciones["relleno"] = 0

    puntuaciones.sort_values(["relleno", 'TOTAL', 'huecos_a_rellenar'], ascending=[True, False, False]).reset_index(
        drop=True).head(3)
    ###asÃ­ es como irÃ¡ el dataframe ordenado en el bucle

    ###DISTRIBUCION
    while puntuaciones.relleno.sum() != puntuaciones.shape[0]:

        puntuaciones = puntuaciones.sort_values(["relleno", 'TOTAL', 'huecos_a_rellenar'],
                                                ascending=[True, False, False]).reset_index(drop=True)
        # display(puntuaciones)
        # los que ya estÃ¡n rellenos se dejan al final, ordenamos por puntuaciones mayores y huecos mayores

        apto = puntuaciones["CD_AEROPUERTOS"].iloc[0]
        hora = puntuaciones["hora"].iloc[0]
        fecha = puntuaciones["fecha"].iloc[0]
        nombre_contenido = puntuaciones["nombre_contenido"].iloc[0]

        salas_c = pd.unique(puntuaciones[puntuaciones["nombre_contenido"] == nombre_contenido].CD_SALAS)

        score = puntuaciones["TOTAL"].iloc[0]
        # se cogen los campos de la primera fila (la que vamos a introducir)
        # se "tacha" esa fila, esto se corresponde al contenido que se ha seleccionado (indice 0) , y a si es necesario ponerlo en entradas salidas se pone en las dos a la vez por las limitaciones del gestor.

        puntuaciones.loc[(puntuaciones.fecha == fecha) & (puntuaciones.hora == hora) & (
                puntuaciones.nombre_contenido == nombre_contenido) & (
                                 puntuaciones.CD_AEROPUERTOS == apto), "relleno"] = 1
        # si el contenido aÃºn no estÃ¡ completo:

        if puntuaciones.loc[0, "huecos_a_rellenar"] != 0:
            # si esa hora-fecha-SALAS no estÃ¡ llena:  (como se emite a la vez , de momento, EXIGIMOS, que no estÃ© relleno ningÃºna de las las salas para un contenido)
            # salas posibles: coincide apto , fecha y hora, y se cogen las salas del contenido
            salas_libres = resultados[
                (resultados.CD_AEROPUERTOS == apto) & (resultados.fecha == fecha) & (resultados.hora == hora) & (
                    resultados.CD_SALAS.isin(salas_c))].nombre_contenido
            salas_libres = sum(1 for sala in salas_libres if len(sala) < 6)
            if len(salas_c) == salas_libres:  ### si todas las salas estÃ¡n libres:
                resultados["nombre_contenido"] = resultados.apply(
                    lambda x: x.nombre_contenido + [nombre_contenido] if (x.fecha == fecha) & (x.hora == hora) & (
                            x.CD_AEROPUERTOS == apto) else x.nombre_contenido, axis=1)
                resultados["score"] = resultados.apply(
                    lambda x: x.score + [score] if (x.fecha == fecha) & (x.hora == hora) & (
                            x.CD_AEROPUERTOS == apto) else x.score, axis=1)
                puntuaciones["huecos_a_rellenar"] = puntuaciones.apply(lambda x: x.huecos_a_rellenar - 1 if (
                        x.nombre_contenido == nombre_contenido) else x.huecos_a_rellenar, axis=1)

                # se resta 1 a "huecos a rellenar

    # chequeo que todos los contenidos se han rellenado correctamente:
    # flat_list = [item for sublist in resultados.nombre_contenido.to_list() for item in sublist]
    # import collections
    # counter=collections.Counter(flat_list)

    #####OUTPUT JFT
    output = resultados["nombre_contenido"].apply(pd.Series)
    
    try:
        while output.shape[1] < 6:
            output[str(output.shape[1])] = float('nan')
    
        output.columns = ["1_cont", "2_cont", "3_cont", "4_cont", "5_cont", "6_cont"]
        output["fecha"] = resultados["fecha"]
        output["hora"] = resultados["hora"]
        output = output.drop_duplicates()  ### quito los duplicados salas
    
        output1 = output[["fecha", "hora", "1_cont"]].rename(columns={"1_cont": "nombre_contenido"})
        output2 = output[["fecha", "hora", "2_cont"]].rename(columns={"2_cont": "nombre_contenido"})
        output3 = output[["fecha", "hora", "3_cont"]].rename(columns={"3_cont": "nombre_contenido"})
        output4 = output[["fecha", "hora", "4_cont"]].rename(columns={"4_cont": "nombre_contenido"})
        output5 = output[["fecha", "hora", "5_cont"]].rename(columns={"5_cont": "nombre_contenido"})
        output6 = output[["fecha", "hora", "6_cont"]].rename(columns={"6_cont": "nombre_contenido"})
    
        output = pd.concat([output1, output2, output3, output4, output5, output6]).dropna()
        output = output.sort_values(["nombre_contenido", 'fecha', 'hora'], ascending=[True, True, True]).reset_index(
            drop=True)
        output = pd.merge(output, puntuaciones[["nombre_contenido", "pases_hora"]].drop_duplicates(), on="nombre_contenido",
                          how="left")
        # display(output.head(50))
    
        output_jft = output.copy()
    
        week = ['L', 'Ma', 'Mi', 'J', 'V', 'S', 'D']
        output_jft["fecha_dia"] = output_jft["fecha"]
        output_jft["fecha"] = output_jft["fecha"].apply(lambda x: week[x.weekday()])
        #output_jft['fecha'] = output_jft[['fecha', "hora", "nombre_contenido", "pases_hora"]].groupby(
        #    ["hora", "nombre_contenido", "pases_hora"]).transform(lambda x: ';'.join(x))

        output_jft = output_jft.drop_duplicates().sort_values(["nombre_contenido","fecha_dia", "hora"])
        # display(output_jft.sort_values(["nombre_contenido","hora"]).head(15))
        # semana fecha inicio y fin semana
        # append
    
        # fecha carga dia hora.
    
        conten = client.query("""
        SELECT * FROM `JFT_DATOS_PROD""" + libreria + """.Maestro_contenidos_caracteristicas` 
        where CD_AEROPUERTOS  like '%""" + AEROPUERTO + """%'""" ).to_dataframe()
    
        output_jft = pd.merge(left=output_jft, right=conten[['nombre_contenido', "CD_AEROPUERTOS", "CD_SALAS"]],
                              on=['nombre_contenido'], how='inner')
        output["inicio_calendario_campaign"] = puntuaciones["inicio_calendario_campaign"].iloc[0]
        output["fin_calendario_campaign"] = puntuaciones["fin_calendario_campaign"].iloc[0]
    
        output_jft["inicio_calendario_campaign"] = puntuaciones["inicio_calendario_campaign"].iloc[0]
        output_jft["fin_calendario_campaign"] = puntuaciones["fin_calendario_campaign"].iloc[0]
    
        output["fecha_carga"] = datetime.now()
        output_jft["fecha_carga"] = datetime.now()
        
        output['fecha_carga'] = output['fecha_carga'].dt.strftime('%Y-%m-%d')
        output["CD_AEROPUERTOS"] = AEROPUERTO
        output_jft["CD_AEROPUERTOS"] = AEROPUERTO
        output['fecha'] = pd.to_datetime(output['fecha'])
        output['fecha'] = output['fecha'].dt.strftime('%Y-%m-%d')

        output['hora'] = output['hora'].astype(str)

        output['inicio_calendario_campaign'] = pd.to_datetime(output['inicio_calendario_campaign'])
        output['inicio_calendario_campaign'] = output['inicio_calendario_campaign'].dt.strftime('%Y-%m-%d')

        output['fin_calendario_campaign'] = pd.to_datetime(output['fin_calendario_campaign'])
        output['fin_calendario_campaign'] = output['fin_calendario_campaign'].dt.strftime('%Y-%m-%d')


        output_jft = output_jft.drop_duplicates()
        output= output[['nombre_contenido',
                'fecha','fecha_carga',
                'hora','pases_hora',
                'inicio_calendario_campaign',
                'fin_calendario_campaign', 
                'CD_AEROPUERTOS']]
    
        
        
        
        
        output.to_gbq("JFT_CR_PROD" + libreria + ".output", "jft-content-recommender", if_exists="append")

    except:
        output_jft = 0
        print('tabla vacia')
    return output_jft


def reparto(**kwargs):
    
    client = bigquery.Client(project="jft-content-recommender")
    aeropuertos = por_aeropuerto()
    truncate_tabla = client.query(""" TRUNCATE TABLE jft-content-recommender.JFT_CR_PROD.output_jft """)
     
    for a in aeropuertos.CD_AEROPUERTOS.unique():
        
        try:
            reparto_por_aeropuerto = reparto_aeropuerto(a)
        
            if type(reparto_por_aeropuerto) ==pd.core.frame.DataFrame:
                output_jft = reparto_por_aeropuerto.copy()
                output_jft['inicio_calendario_campaign'] = pd.to_datetime(output_jft['inicio_calendario_campaign'])
                output_jft['inicio_calendario_campaign'] = output_jft['inicio_calendario_campaign'].dt.strftime('%Y-%m-%d')
            
                output_jft['fin_calendario_campaign'] = pd.to_datetime(output_jft['fin_calendario_campaign'])
                output_jft['fin_calendario_campaign'] = output_jft['fin_calendario_campaign'].dt.strftime('%Y-%m-%d')
                output_jft.to_gbq("JFT_CR_PROD" + libreria + ".output_jft",
                                  "jft-content-recommender",
                                  if_exists="append") 

            else:
                print('tabla vacia')
        except:
            print('tabla vacia') 
            
# modulo anade en automatico calendarios

#aÃ±adir calendario general


def crea_calendario_general(id_camp,inicio,fin):
  ruta_calendar = r'http://apiweb.admira.com/v1/campaign/' + str(id_camp) + r'/date?access_token=xgVxuFI3Ng9qTy9NHdoYchm9aBYan8VbCjwG38Ff'
  payload = {'project_id': 2, 
        'dateStart': inicio,
        'dateEnd': fin}
  response = requests.post(ruta_calendar, data=payload)
  status = response.json()

  if status['status'] == 'success':
    id_calendar= status['data']['id']
  else:
    print(response.text)
  return id_calendar      

# Al calendario general aÃ±adir posibles horas emision

def horas_calendario_general(id_camp, id_calendar, hora, semana):
  ruta_calendar = r'http://apiweb.admira.com/v1/campaign/' + str(id_camp) + r'/date/' + str(id_calendar) + '/schedule?access_token=xgVxuFI3Ng9qTy9NHdoYchm9aBYan8VbCjwG38Ff'
  payload = {'project_id': 2, 
        'hourStart': hora,
        'hourEnd': hora + 60,
        'week': semana}
  response = requests.post(ruta_calendar, data=payload)
  status = response.json()
  id_calendar_sche = status['data']['id']
  print(id_calendar_sche)
  return id_calendar_sche
            
# recuperamos el id camp conten
def recupera_id_calendar_conten(id_camp):
  ruta_camp_conten = 'http://apiweb.admira.com/v1/campaign/' + str(id_camp) + '/content?access_token=xgVxuFI3Ng9qTy9NHdoYchm9aBYan8VbCjwG38Ff'
  response = requests.get(ruta_camp_conten)
  response2 = response.json()
  campaignContent = response2['data'][0]['id']
  print(campaignContent)
  print(response.text)
  return campaignContent

#aÃ±adir calendario contenido
def subcalendario_conten(id_camp,campaignContent,id_calendar):
  ruta_calendar = r'http://apiweb.admira.com/v1/campaign/' + str(id_camp) + r'/content/' + str(campaignContent) + r'/date?access_token=xgVxuFI3Ng9qTy9NHdoYchm9aBYan8VbCjwG38Ff'
  payload = {'campaign_date_id': id_calendar, 
        'dateStart': 0}
  response = requests.post(ruta_calendar, data=payload)
  response2 = response.json()
  id_calendar_conten = response2['data']['id']
  print(id_calendar_conten)

  print(response.text)
  return id_calendar_conten


  id_calendar_conten = subcalendario_conten(id_camp,campaignContent,id_calendar)
  
#aÃ±adir horas al calendario contenido
def subcalendario_conten_horas(id_camp,campaignContent,id_calendar_conten, id_calendar_sche, semana, hora):
  ruta_calendar_hour = (r'http://apiweb.admira.com/v1/campaign/' 
                    + str(id_camp) 
                    + r'/content/' 
                    + str(campaignContent) 
                    + r'/date/'
                    + str(id_calendar_conten)
                    + r'/schedule?access_token=xgVxuFI3Ng9qTy9NHdoYchm9aBYan8VbCjwG38Ff')
  payload = {'campaign_schedule_id': id_calendar_sche, 
            'hourStart': hora,
            'hourEnd': hora + 60,
            'week': semana}

  response = requests.post(ruta_calendar_hour, data=payload)
  print(response.text)
  return response.text  

def cambiamos_a_formato_epoch(fecha_numerica):
  '''
  Tranforma una variable de entrada fecha  con formato numerico en 
  yyymmdd a un formato numerico epoch
  input: numero de una fecha con formato yyymmdd
  output: numero fecha epoch

  '''

  dt = datetime(int(str(fecha_numerica)[0:4]), 
                        int(str(fecha_numerica)[4:6]),
                        int(str(fecha_numerica)[6:8]),
                        0, 0, 0, 0) 
  return int(dt.timestamp())


def crea_df_array_fechas(fecha_inicio,fecha_fin):
  start = datetime.strptime(fecha_inicio, "%Y-%m-%d")
  end = datetime.strptime(fecha_fin, "%Y-%m-%d")
  date_generated = pd.date_range(start, end)
  df_date_generated = pd.DataFrame(date_generated,columns = ['fechas_arr'])
  df_date_generated['dia_semana'] = df_date_generated['fechas_arr'].dt.day_name()
  df_date_generated['dia_semana_num'] = df_date_generated['fechas_arr'].dt.weekday
  return df_date_generated
  
def buscamos_param_week(hora_entrada,camp_calendar, df_date_generated ):

    particion_bucle_hora = camp_calendar[camp_calendar['hora'] == hora_entrada]
    particion_bucle_hora = particion_bucle_hora[['fecha_dia']]
    particion_bucle_hora['fecha_dia'] = pd.to_datetime(particion_bucle_hora['fecha_dia'])
    particion_bucle_hora = particion_bucle_hora.drop_duplicates()
    particion_bucle_hora_paso2 = df_date_generated.merge(particion_bucle_hora, left_on='fechas_arr', right_on='fecha_dia', how='left')
    particion_bucle_hora_paso2['check'] = np.where(particion_bucle_hora_paso2['fechas_arr'] ==particion_bucle_hora_paso2['fecha_dia'] ,'1','0')
    particion_bucle_hora_paso2 = particion_bucle_hora_paso2.sort_values('dia_semana_num')
    lista_dias = pd.DataFrame([0,1,2,3,4,5,6], columns = ['dia_semana_num'])
    particion_bucle_hora_paso2_b = lista_dias.merge(particion_bucle_hora_paso2, left_on='dia_semana_num', right_on='dia_semana_num', how='left')
    particion_bucle_hora_paso2_b["check"] = particion_bucle_hora_paso2_b["check"].fillna('0')
    particion_bucle_hora_paso3 = particion_bucle_hora_paso2_b[['check']]
    particion_bucle_hora_paso3['agrupa'] = 1
    # concatenate the string
    particion_bucle_hora_paso3['week'] = particion_bucle_hora_paso3.groupby(['agrupa'])['check'].transform(lambda x : ' '.join(x))
    particion_bucle_hora_paso3 = particion_bucle_hora_paso3[['week']].drop_duplicates().values.tolist()
    week = particion_bucle_hora_paso3[0][0].replace(' ', '')

    return week

def sacamos_datos_de_recomendador(**kwargs):
    
    client = bigquery.Client(project="jft-content-recommender")
        
    camp_calendar = client.query('''
    SELECT distinct a.*, a.nombre_contenido || '_' || a.CD_AEROPUERTOS as nombre_campaign
    FROM `jft-content-recommender.JFT_CR_PROD.output_jft` a

      ''').to_dataframe()
    ruta_camp = 'http://apiweb.admira.com/v1/campaign?&access_token=xgVxuFI3Ng9qTy9NHdoYchm9aBYan8VbCjwG38Ff'
    response = requests.get(ruta_camp)
    response2 = response.json()
    response3 = response2['data']
    
    lista_camp =[]
    for a in response3:
        if a['state']['name'] in ['delete','canceled','expired']:
            pass
        else:
            linea = [a['id'],a['name'],a['state']['name']]
            lista_camp.append(linea)
    df_camp = pd.DataFrame(lista_camp,columns = ['campaign_id','nombre_campaign', 'estado_campaign'])
    output_camp = camp_calendar.merge(df_camp, left_on='nombre_campaign', right_on='nombre_campaign', how='left') 
         
    return output_camp

def creamos_los_calendarios(**kwargs):

    camp_calendar = sacamos_datos_de_recomendador()
    camp_calendar = camp_calendar.dropna()
    
    if len(camp_calendar.values.tolist()) == 0:
        
        print('no hay calendarios nuevos')
        
    else:
        print('ok')
    
        unique_list_camp = camp_calendar[['campaign_id','CD_AEROPUERTOS']].drop_duplicates().values.tolist()
        
        for camp_id_aeropuerto in unique_list_camp:
          camp_id = camp_id_aeropuerto[0]
          aeropuerto = camp_id_aeropuerto[1]
        
          # Sacamos los datos de la campaÃ±a
          particion_bucle = camp_calendar[(camp_calendar.campaign_id == camp_id) & (camp_calendar.CD_AEROPUERTOS == aeropuerto)]
          #Fecha inicio camp
          fecha_inicio = (list(particion_bucle['inicio_calendario_campaign'].unique()))[0]
          fecha_inicio_int = fecha_inicio.replace('-', '')
          fecha_inicio_epoch =   cambiamos_a_formato_epoch(fecha_inicio_int)
          #Fecha fin camp
          fecha_fin = (list(particion_bucle['fin_calendario_campaign'].unique()))[0]
          fecha_fin_int = fecha_fin.replace('-', '')
          fecha_fin_epoch = cambiamos_a_formato_epoch(fecha_fin_int)
        
          # creamos calendario general
          id_calendar = crea_calendario_general(camp_id,fecha_inicio_epoch,fecha_fin_epoch)
          # sacamos id camp conten
          campaignContent = recupera_id_calendar_conten(camp_id)
          # creamos dataframe de las fechas de la campaÃ±a
          df_date_generated = crea_df_array_fechas(fecha_inicio,fecha_fin)
        
          horas = (list(particion_bucle['hora'].unique()))
          for hora in horas:
            particion_bucle_hora = camp_calendar[camp_calendar['hora'] == hora]
            week = buscamos_param_week(hora,camp_calendar, df_date_generated )
            id_calendar_sche = horas_calendario_general(camp_id, id_calendar, int(hora[0:2]) *60, week)
            id_calendar_conten = subcalendario_conten(camp_id,campaignContent,id_calendar)
            subcalendario_conten_horas(camp_id,campaignContent,id_calendar_conten, id_calendar_sche, week, int(hora[0:2]) *60)
        
        

default_args = {
    'start_date': datetime(2022, 7, 5),
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}
if entorno == 'TEST':
    
    with models.DAG(
        'recomender_TEST',
        default_args=default_args,
        description='liveness monitoring dag_TEST',
        schedule_interval=r'0 6 * * 1') as dag: ## lunes a las 6 (hora google)
    
        load_caracteristicas_semana_TEST = BigQueryOperator(
            dag=dag,  # need to tell airflow that this task belongs to the dag we defined above
            task_id='load_caracteristicas_semana_TEST',  # task id's must be uniqe within the dag
            bql=
            '''
           CALL `jft-content-recommender.JFT_FUNCIONES''' + libreria + '''.tratamiento_semanal`();
             ''',
            use_legacy_sql=False
        )
    
        calculo_puntuaciones_TEST = python_operator.PythonOperator(
            task_id='calculo_puntuaciones_TEST',
            #op_kwargs={"path": gs},
            provide_context=True,
            python_callable=puntuaciones
        )
        reparto_contenidos_TEST = python_operator.PythonOperator(
            task_id='reparto_contenidos_TEST',
            #op_kwargs={"path": gs},
            provide_context=True,
            python_callable=reparto
        )
    
        clean_output_TEST= BigQueryOperator(
            dag=dag,  # need to tell airflow that this task belongs to the dag we defined above
            task_id='clean_output_TEST',  # task id's must be uniqe within the dag
            bql=
            '''
           CALL `jft-content-recommender.JFT_FUNCIONES''' + libreria + '''.clean_tablas_recomendador`();
             ''',
            use_legacy_sql=False
        )
    
        creamos_calendarios_etl_test = python_operator.PythonOperator(
            task_id='creamos_calendarios_etl_test',
            #op_kwargs={"path": gs},
            provide_context=True,
            python_callable=creamos_los_calendarios
        )
        ##reset_pesos>> \
        load_caracteristicas_semana_TEST >> calculo_puntuaciones_TEST >> reparto_contenidos_TEST >> clean_output_TEST >> creamos_calendarios_etl_test
else:
    
    with models.DAG(
        'recomender',
        default_args=default_args,
        description='liveness monitoring dag',
        schedule_interval=r'0 6 * * 1') as dag: ## lunes a las 6 (hora google)
    
        load_caracteristicas_semana = BigQueryOperator(
            dag=dag,  # need to tell airflow that this task belongs to the dag we defined above
            task_id='load_caracteristicas_semana',  # task id's must be uniqe within the dag
            bql=
            '''
           CALL `jft-content-recommender.JFT_FUNCIONES''' + libreria + '''.tratamiento_semanal`();
             ''',
            use_legacy_sql=False
        )
    
        calculo_puntuaciones = python_operator.PythonOperator(
            task_id='calculo_puntuaciones',
            #op_kwargs={"path": gs},
            provide_context=True,
            python_callable=puntuaciones
        )
        reparto_contenidos = python_operator.PythonOperator(
            task_id='reparto_contenidos',
            #op_kwargs={"path": gs},
            provide_context=True,
            python_callable=reparto
        )
    
        clean_output= BigQueryOperator(
            dag=dag,  # need to tell airflow that this task belongs to the dag we defined above
            task_id='clean_output',  # task id's must be uniqe within the dag
            bql=
            '''
           CALL `jft-content-recommender.JFT_FUNCIONES''' + libreria + '''.clean_tablas_recomendador`();
             ''',
            use_legacy_sql=False
        )
        
        creamos_calendarios_etl = python_operator.PythonOperator(
            task_id='creamos_calendarios_etl',
            #op_kwargs={"path": gs},
            provide_context=True,
            python_callable=creamos_los_calendarios
        )
    
        ##reset_pesos>> \
        load_caracteristicas_semana >> calculo_puntuaciones >> reparto_contenidos >> clean_output >> creamos_calendarios_etl

    # reset_pesos = BigQueryOperator(
    #    dag=dag,  # need to tell airflow that this task belongs to the dag we defined above
    #   task_id='reset_pesos',  # task id's must be uniqe within the dag
    #    bql=
    #   '''
    #   #SELECT * FROM jft-content-recommender.JFT_DATOS_RAW''' + libreria + '''.Datos_excel_pesos_raw
    #     ''',
    #    destination_dataset_table='jft-content-recommender.JFT_DATOS_PROD''' + libreria + '''.Datos_excel_pesos',
    #   write_disposition='WRITE_TRUNCATE',
    #   use_legacy_sql=False
    #)
